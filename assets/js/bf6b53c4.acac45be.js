"use strict";(self.webpackChunkalgorithmic_business_forecasting=self.webpackChunkalgorithmic_business_forecasting||[]).push([[5650],{8963:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>a,toc:()=>d});var n=o(4848),r=o(8453);const i={sidebar_label:"(Algorithmic) LSTM Neural Net",sidebar_position:4},s="Long Short Term Memory Neural Network: LSTM",a={id:"models/model_4",title:"Long Short Term Memory Neural Network: LSTM",description:"How It Works",source:"@site/docs/models/model_4.md",sourceDirName:"models",slug:"/models/model_4",permalink:"/algorithmic-business-forcasting/docs/models/model_4",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/models/model_4.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_label:"(Algorithmic) LSTM Neural Net",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"(Algorithmic) Random Forest",permalink:"/algorithmic-business-forcasting/docs/models/model_3"},next:{title:"Results",permalink:"/algorithmic-business-forcasting/docs/results"}},l={},d=[{value:"How It Works",id:"how-it-works",level:2},{value:"Why We Think It&#39;s Good",id:"why-we-think-its-good",level:2}];function c(e){const t={h1:"h1",h2:"h2",p:"p",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"long-short-term-memory-neural-network-lstm",children:"Long Short Term Memory Neural Network: LSTM"}),"\n",(0,n.jsx)(t.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,n.jsx)(t.p,{children:"Neural networks are the second key representative of \u201cAlgorithmic Modeling\u201d mentioned\nby Breiman in \u201cStatistical Modeling: The Two Cultures(Breiman 2003)\u201d, of which recurrent neural networks are specifically designed for time series analysis. The long short-term\nmemory network(Hochreiter and Schmidhuber 1997), is an adjustment of the basic recurrent neural network, specifically on how hidden states and calculated and used, which\nallows the model to retain \u201cmemory\u201d of previous time steps over extended time intervals."}),"\n",(0,n.jsx)(t.h2,{id:"why-we-think-its-good",children:"Why We Think It's Good"}),"\n",(0,n.jsx)(t.p,{children:"This capability of retaining information on data from hundreds or thousands of timestamps\nin the past makes LSTMs a good candidate for forecasting recession years, which are often\nsignificantly spaced apart and independent of seasonality. As a variant of recurrent neural\nnetworks, it is also capable of taking in an arbitrary number of inputs at each time step,\nallowing it to take advantage of the variance present in explanatory features, combining the\nbenefits of both OLS and ARIMA models, making it the optimal candidate for our problem."})]})}function m(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,t,o)=>{o.d(t,{R:()=>s,x:()=>a});var n=o(6540);const r={},i=n.createContext(r);function s(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);